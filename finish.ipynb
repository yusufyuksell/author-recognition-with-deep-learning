{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e474337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from snowballstemmer import TurkishStemmer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn import preprocessing\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "407822b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('trmodel', binary=True)\n",
    "model = Word2Vec(sentences=common_texts, vector_size=100,\n",
    "                 window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b855cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Yazarlar'\n",
    "authors = glob.glob(path + \"/*\")\n",
    "\n",
    "all_files = glob.glob(path + \"/*.txt\")\n",
    "count = 0\n",
    "encodedtext = np.zeros((2000,400))\n",
    "\n",
    "label = ()\n",
    "for i in range(1, 21):\n",
    "    label += (i,) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51ca5132",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "for h in range(20):\n",
    "    for text in glob.glob(authors[h], recursive=True):\n",
    "       \n",
    "        \n",
    "        all_files = glob.glob(text + \"/*.txt\")\n",
    "    for i in all_files:\n",
    "        punctuiations = \"!#$%&'()*+,-./:â€™;<=>?@[\\]^_`{|}~'\"\n",
    "        total = np.zeros((1,400))\n",
    "        textfile = open(i,'r',encoding=\"utf-8\")\n",
    "        str = textfile.read()\n",
    "        no_punct = \" \"\n",
    "        \n",
    "        for char in str:\n",
    "            if(char not in punctuiations):\n",
    "                no_punct = no_punct + char\n",
    "        stop_words = stopwords.words('turkish')\n",
    "        tokenize_words = word_tokenize(no_punct)\n",
    "        tokenize_words_without_stopwords = []\n",
    "        \n",
    "        for word in tokenize_words:\n",
    "            if word not in stop_words:\n",
    "                tokenize_words_without_stopwords.append(word)\n",
    "        stammer = TurkishStemmer()\n",
    "        input_str = tokenize_words_without_stopwords\n",
    "\n",
    "        \n",
    "        \n",
    "        for word in input_str:\n",
    "            try:\n",
    "                #print(stammer.stemWord(word))\n",
    "                total += (word_vectors.get_vector(stammer.stemWord(word).lower()))\n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        total = preprocessing.normalize(total)\n",
    "        total = total.reshape(400)\n",
    "        encodedtext[count,:] = total\n",
    "        count += 1    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    #col = np.array([calc(label) for aut in encodedtext[:, 1].astype(np.int32())])      \n",
    "    arr = np.column_stack((encodedtext, label))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b17c1ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('sonuc.npy', arr, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afda5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "encodedtext = iris.data\n",
    "label = iris.target\n",
    "\n",
    "encodedtext_train, encodedtext_test, label_train, label_test = train_test_split(encodedtext, label, random_state = 3)\n",
    "\n",
    "# training a DescisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree_model = DecisionTreeClassifier(max_depth = 2).fit(encodedtext_train, label_train)\n",
    "dtree_predictions = dtree_model.predict(encodedtext_test)\n",
    "\n",
    "cm = confusion_matrix(label_test, dtree_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
